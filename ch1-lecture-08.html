<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <title>Lecture 8 — Cayley–Hamilton Theorem & Diagonalization</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }
    h2, h3 { color: darkblue; }
    pre { background: #f4f4f4; padding: 10px; border-radius: 5px; }
  </style>
</head>
<body>

<h1>Lecture 8 — Cayley–Hamilton Theorem & Diagonalization</h1>

<!-- ================== Cayley–Hamilton ================== -->
<h2>1. Cayley–Hamilton Theorem</h2>

<h3>Definition</h3>
<p>
The <b>Cayley–Hamilton theorem</b> states that every square matrix satisfies its own characteristic equation.
</p>
<p>
For a square matrix \(A\) of order \(n\), with characteristic polynomial
\[
p(\lambda) = \det(A - \lambda I) = \lambda^n + c_{1}\lambda^{n-1} + \cdots + c_{n},
\]
then
\[
p(A) = A^n + c_{1}A^{n-1} + \cdots + c_{n}I = 0.
\]
</p>

<h3>Properties</h3>
<ul>
  <li>It provides a relation between powers of the matrix.</li>
  <li>Useful in computing \(A^{-1}\), \(A^k\), and simplifying polynomials of \(A\).</li>
  <li>Holds for all square matrices (real or complex).</li>
</ul>

<h3>Examples</h3>

<h4>Example 1 (2×2)</h4>
<p>Let 
\[
A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
\]
Characteristic polynomial:
\[
\det(A-\lambda I) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
\]
So by Cayley–Hamilton:
\[
A^2 - 4A + 3I = 0.
\]
</p>

<h4>Example 2 (3×3)</h4>
<p>Let 
\[
A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{bmatrix}.
\]
Characteristic polynomial:
\[
(\lambda-1)(\lambda-2)(\lambda-3) = \lambda^3 - 6\lambda^2 + 11\lambda - 6.
\]
So:
\[
A^3 - 6A^2 + 11A - 6I = 0.
\]
</p>

<h4>Example 3 (3×3)</h4>
<p>Let 
\[
A = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}.
\]
Characteristic polynomial:
\[
\det(A-\lambda I) = -\lambda^3 + 1.
\]
So:
\[
A^3 - I = 0.
\]
</p>

<!-- ================== Diagonalization ================== -->
<h2>2. Diagonalization</h2>

<h3>Definition</h3>
<p>
A square matrix \(A\) is said to be <b>diagonalizable</b> if there exists an invertible matrix \(P\) and a diagonal matrix \(D\) such that
\[
A = P D P^{-1}.
\]
Here, \(D\) contains eigenvalues of \(A\) on the diagonal, and \(P\) contains the corresponding eigenvectors.
</p>

<h3>Properties</h3>
<ul>
  <li>A matrix with \(n\) distinct eigenvalues is always diagonalizable.</li>
  <li>Symmetric (real) matrices are always diagonalizable with orthogonal eigenvectors.</li>
  <li>Diagonalization simplifies computing powers: \(A^k = P D^k P^{-1}\).</li>
</ul>

<h3>Examples</h3>

<h4>Example 1 (2×2)</h4>
<p>
\[
A = \begin{bmatrix} 4 & 1 \\ 0 & 2 \end{bmatrix}.
\]
Characteristic polynomial:
\((\lambda-4)(\lambda-2)\).  
Eigenvalues: \(\lambda_1=4, \lambda_2=2\).  
Eigenvectors: \([1,0]^T\), \([1,-2]^T\).  
So
\[
P = \begin{bmatrix} 1 & 1 \\ 0 & -2 \end{bmatrix}, \quad
D = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}.
\]
</p>

<h4>Example 2 (3×3)</h4>
<p>
\[
A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 4 \end{bmatrix}.
\]
Already diagonal: \(D = A\).  
\(P = I\).
</p>

<h4>Example 3 (3×3)</h4>
<p>
\[
A = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \end{bmatrix}.
\]
Eigenvalues: \(\lambda=1\) (multiplicity 2), \(\lambda=2\).  
Eigenvectors: \([1,0,0]^T, [0,1,0]^T, [0,0,1]^T\).  
So
\[
P = I, \quad D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \end{bmatrix}.
\]
</p>

  <footer style="background:#f8f9fa;color:#333;text-align:center;padding:10px;font-size:13px;font-style:italic;border-top:1px solid #ccc;">
© 2025 | Designed by Dr. Surnam Narendra
</footer>
</body>
</html>
